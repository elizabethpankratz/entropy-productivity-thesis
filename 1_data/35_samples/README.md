**Directories:**
- `1_raw_samples/`: Samples from DECOW16B of size max. 100,000 based on the queries in `queries.csv`. (Not on GitHub because of size; files available upon request.)
  - Created in `1_sample_sfxs.py`.
- `2_random_subsamples/`: Randomly selected subsamples of the files in `raw_samples/` of size max. 20,000.
  - Created in `2_count_derivs_and_bases.py`.
- `2_backform_samples_nano/`: Backformed base candidates generated for each of the random subsamples, including frequencies of derivations and candidate bases in DECOW16A-NANO.
- `2_backform_samples_nano_annot/`: Annotated copies of the files in `2_backform_samples_nano/`, still in need of some postprocessing.
  - Created in `2_count_derivs_and_bases.py`.
- `6_backform_base_cutoff/`: Annotated bases with manually-queried bases and duplicated lemmas merged. Called "cutoff" because only bases that that have a frequency > 0 and that I annotated as true bases are considered, and only their corresponding lemmas are taken as true derivations.
  - Created in `6_merge_manual_queries.ipynb`.
- `7_analysis_samples_unclean/`: Subsets of the samples in `2_random_subsamples/`, selected based on whether the derivation is annotated as a true derivation in the respective file in `6_backform_base_cutoff/`.
  - Created in `7_create_7_analysis_samples.ipynb`
- `7_analysis_samples/`: Samples used for all analyses. Manually cleaned based on the files in `7_analysis_samples_unclean/`. Cleaning involved:
  - Lemmatisation of any last NN compounds
  - Orthographic normalisation of umlauts

**Scripts:**
- `1_sample_sfxs.py` (run on [SeaCOW](https://github.com/rsling/seacow) server):
  - In: `queries.csv`
  - Out: contents of `1_raw_samples/`
- `2_count_derivs_and_bases.py` (run on SeaCOW server):
  - In: `backformer_one.py`
  - Out: contents of `2_random_subsamples/` and `2_backform_samples_nano/`
- `3_check_annotations.ipynb` (run locally):
  - In: contents of `2_backform_samples_nano/`
  - Out: nothing.
- `4_prep_manual_query.ipynb` (run locally):
  - In: contents of `2_backform_samples_nano/`
  - Out: `bases_manual_query.csv`
- `5_manual_query.py` (run on SeaCOW server):
  - In: `bases_manual_query.csv`
  - Out: `bases_manual_query_done.csv`
- `6_merge_manual_queries.ipynb` (run locally):
  - In: `bases_manual_query_done.csv`, contents of `2_backform_samples_nano/`
  - Out: contents of `6_backform_base_cutoff/`
- `7_create_analysis_samples.ipynb` (run locally):
  - In: `backformer_one.py`, contents of `2_random_subsamples/`, contents of `6_backform_base_cutoff/`
  - Out: contents of `7_analysis_samples_unclean/`

**Modules:**
- `backformer_one.py`: Version 1 of module containing functions for generating candidate bases for the given suffix. Based on rules in [DErivBase](https://www.ims.uni-stuttgart.de/en/research/resources/lexica/derivbase/), used in `2_count_derivs_and_bases.py`.

**Data files:**
- `queries.csv`: Lists the DErivBase rules for each suffix I query, which function in `backformer` takes care of that suffix, and the query used in `1_sample_sfxs.py` to get the samples in `raw_samples/` from DECOW16B.
- `bases_manual_query.csv`: Contains the bases that weren't generated by `backformer` that were manually reconstructed and need to be queried.
- `bases_manual_query_done.csv`: Same as `bases_manual_query.csv` but now including the frequencies (produced by `5_manual_query.py`)
