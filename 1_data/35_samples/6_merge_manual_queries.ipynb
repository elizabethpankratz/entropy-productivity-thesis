{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads in the queried bases from `bases_manual_query_done.csv`, adds new rows with the new base and its frequency into the datasets for the respective suffixes and annotates that row as `true_base == 1`.\n",
    "\n",
    "After adding this information to each dataset, this script also merges the frequences for the lemmas and bases indicated in the `merge` column.\n",
    "\n",
    "It creates a dataset with all pairs removed for which the frequency of the base is 0.\n",
    "This data is saved in `backform_base_cutoff/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done -age\n",
      "Done -ament\n",
      "Done -and\n",
      "Done -ant\n",
      "Done -anz\n",
      "Done -ateur\n",
      "Done -ation\n",
      "Done -ator\n",
      "Done -atur\n",
      "Done -eA\n",
      "Done -el\n",
      "Done -ement\n",
      "Done -end\n",
      "Done -ent\n",
      "Done -enz\n",
      "Done -er\n",
      "Done -eur\n",
      "Done -eV\n",
      "Done -heit\n",
      "Done -ie\n",
      "Done -iker\n",
      "Done -ikum\n",
      "Done -ik\n",
      "Done -iment\n",
      "Done -ismus\n",
      "Done -ist\n",
      "Done -itaet\n",
      "Done -iteur\n",
      "Done -ition\n",
      "Done -itur\n",
      "Done -ium\n",
      "Done -ling\n",
      "Done -nis\n",
      "Done -schaft\n",
      "Done -ung\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "ANNOT_FILES = [fn for fn in os.listdir('2_backform_samples_nano_annot') if fn[-14:] == 'base_annot.csv']\n",
    "SFXS = [fn.split('_')[0] for fn in ANNOT_FILES]\n",
    "MANUAL_FREQS = pd.read_csv('bases_manual_query_done.csv')\n",
    "\n",
    "for idx in range(len(SFXS)):\n",
    "    \n",
    "    curr_file = ANNOT_FILES[idx]\n",
    "    curr_sfx = SFXS[idx]\n",
    "    curr_df = pd.read_csv('backform_samples_nano_annot/' + curr_file)\n",
    "    curr_df = curr_df.drop(columns=['query_by_hand', 'query_pos'])\n",
    "    \n",
    "    # First, we add in the frequency from the manually queried bases.\n",
    "    # Subset the MANUAL_FREQS dataset for only the current suffix. If there is any content in that subset,\n",
    "    # concatenate with curr_df.\n",
    "    curr_manfreq = MANUAL_FREQS[MANUAL_FREQS.sfx == curr_sfx]\n",
    "    \n",
    "    if len(curr_manfreq) > 0:\n",
    "        \n",
    "        # Drop cols we no longer need (related to original data or manual querying)\n",
    "        # and rename columns with the new values to the names in the original data.\n",
    "        curr_manfreq = curr_manfreq.drop(columns=['unique_candidates', 'pos', 'sfx', 'cql']).rename(columns={'query_by_hand':'unique_candidates', 'query_pos':'pos'})\n",
    "        curr_manfreq['true_base'] = 1\n",
    "        \n",
    "        # Reorder cols according to their order in curr_df so that pd.concat() doesn't complain, and concatenate.\n",
    "        curr_manfreq = curr_manfreq.reindex(columns=curr_df.columns)\n",
    "        curr_df = pd.concat([curr_manfreq, curr_df])\n",
    "    \n",
    "    # Next, we want to sum up the lemma_freq and base_freq values for the rows in which the merge value\n",
    "    # is equal to the lemma value, and add those values to the values beside lemma.\n",
    "    mg_subdf = curr_df[~curr_df['merge'].isna()]\n",
    "    \n",
    "    if len(mg_subdf) > 0:\n",
    "        for idx, row in mg_subdf.iterrows():\n",
    "            \n",
    "            # Get the important values from this row.\n",
    "            target_lem = row['merge']\n",
    "            lem_freq = row.lemma_freq\n",
    "            bas_freq = row.base_freq\n",
    "\n",
    "            # We should be able to ID the correct row to add these values to by looking at whether both true_lemma and true_base\n",
    "            # have values of 1 (since now all the manually-queried bases should also have true_base values of 1)\n",
    "\n",
    "            # So, find the index of the one row that meets this description, use .at to augment the values in curr_df\n",
    "            # in that row and the correct columns.\n",
    "            target_row = curr_df[(curr_df.lemma == target_lem) & (curr_df.true_lemma == 1) & (curr_df.true_base == 1)].index\n",
    "            if len(target_row) == 0:\n",
    "                target_row = curr_df[(curr_df.manual_lemma == target_lem) & (curr_df.true_lemma == 1) & (curr_df.true_base == 1)].index\n",
    "            assert len(target_row) == 1, print(curr_sfx, '\\t', target_lem, '\\t', target_row)\n",
    "\n",
    "            curr_df.at[target_row[0], 'lemma_freq'] += lem_freq\n",
    "            curr_df.at[target_row[0], 'base_freq'] += bas_freq\n",
    "\n",
    "        # Remove the row indices in mg_subdif from curr_df and reset_index.\n",
    "        curr_df = curr_df[curr_df['merge'].isna()].reset_index(drop=True)\n",
    "    \n",
    "    # Drop the merge column; no longer needed.\n",
    "    curr_df = curr_df.drop(columns=['merge'])\n",
    "    \n",
    "    # Finally, wherever base_freq == 0, set true_lemma == 0 and true_base == np.nan.\n",
    "    # We will not consider pairs in which the base never appears.\n",
    "    # Save in backform_base_cutoff/.\n",
    "    curr_df.loc[curr_df.base_freq == 0, 'true_lemma'] = 0\n",
    "    curr_df.loc[curr_df.base_freq == 0, 'true_base'] = pd.np.nan\n",
    "    curr_df.to_csv('6_backform_base_cutoff/' + curr_sfx + '_bases.csv', index=False)\n",
    "    \n",
    "    print('Done', curr_sfx)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
