{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstraps samples from the large samples of *-heit*, *-nis*, and *-schaft*.\n",
    "\n",
    "(This code is clunky, the functions are pretty unnecessary.\n",
    "I rewrote this script for bootstrapping the later samples, and that sleeker code can be found at `../4_applicability/gen_bootstrap_samples.ipynb`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import entropy\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SFXS = ['heit', 'nis', 'schaft']\n",
    "heit = pd.read_csv('../1_data/large_samples/heit.csv')\n",
    "nis = pd.read_csv('../1_data/large_samples/nis.csv')\n",
    "schaft = pd.read_csv('../1_data/large_samples/schaft.csv')\n",
    "all_sfxs_df = pd.concat([heit, nis, schaft], keys=SFXS).reset_index().rename(columns={'level_0':'sfx', 'level_1':'orig_idx'})\n",
    "\n",
    "# Set this flag for whether to sample with or without replacement.\n",
    "W_REPL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of -heit tokens:\t\t 1706488\n",
      "Number of -nis tokens:\t\t 589279\n",
      "Number of -schaft tokens:\t 795870\n"
     ]
    }
   ],
   "source": [
    "print('Number of -heit tokens:\\t\\t', len(heit))\n",
    "print('Number of -nis tokens:\\t\\t', len(nis))\n",
    "print('Number of -schaft tokens:\\t', len(schaft))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_random_sample(df, size):\n",
    "    \"\"\"\n",
    "    Creates a random subset from the given df of the specified size\n",
    "    \n",
    "    Args:\n",
    "        df: pandas df containing corpus query results\n",
    "        size: integer, size of desired sample\n",
    "    Returns:\n",
    "        pandas df of size 'size'\n",
    "    \"\"\"\n",
    "    rd_idcs = np.random.choice(df.index, size = size, replace = W_REPL)\n",
    "    return df.iloc[rd_idcs].reset_index().rename(columns = {'index':'orig_idx'})\n",
    "\n",
    "\n",
    "def get_sample_freqdist(df):\n",
    "    \"\"\"\n",
    "    Counts the number of types each type occurs in the given sample.\n",
    "    \n",
    "    Args:\n",
    "        df: pandas df containing corpus query results, with types\n",
    "            in column 'lemma'\n",
    "    Returns:\n",
    "        pandas df with columns type, n_tokens, rank\n",
    "    \"\"\"\n",
    "    \n",
    "    # Count the occurrences of values in 'lemma' and add a rank column\n",
    "    df = pd.DataFrame(df.lemma.value_counts()).reset_index().rename(columns={'index':'type', 'lemma':'n_tokens'})\n",
    "    df['rank'] = list(range(1, len(df)+1))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_sample_entropies(freqdist, base=2):\n",
    "    \"\"\"\n",
    "    Calculates the entropy and scaled entropy of a distribution from a sequence of labels.\n",
    "    \n",
    "    Arg:\n",
    "        freqdist: pandas df, output of get_sample_freqdist()\n",
    "        base: base of the logarithm to use in the computation, default 2 (for Shannon entropy)\n",
    "    Returns:\n",
    "        A tuple of floats: raw entropy in bits, and the entropy scaled to [0,1]\n",
    "    \"\"\"\n",
    "    \n",
    "    # Compute entropy, and scale it to [0,1] by dividing by log2 of the number of categories\n",
    "    # (this is the max possible entropy for that number of categories)\n",
    "    ent = entropy(freqdist['n_tokens'], base = base)\n",
    "    scaled_ent = ent/math.log(len(freqdist), 2)\n",
    "    return ent, scaled_ent\n",
    "\n",
    "\n",
    "def get_sample_hapaxes(freqdist):\n",
    "    \"\"\"\n",
    "    Calculates the entropy and scaled entropy of a distribution from a sequence of labels.\n",
    "    \n",
    "    Arg:\n",
    "        freqdist: pandas df, output of get_sample_freqdist()\n",
    "    Returns:\n",
    "        A tuple of numbers: proportion hapaxes/types (float), number of hapaxes (int)\n",
    "    \"\"\"\n",
    "    num_hapaxes = sum(freqdist['n_tokens'] == 1)\n",
    "    propn_hapaxes = num_hapaxes/len(freqdist)\n",
    "    return num_hapaxes, propn_hapaxes\n",
    "\n",
    "\n",
    "# samp = draw_random_sample(nis, 100)\n",
    "# samp_fd = get_sample_freqdist(samp)\n",
    "# get_sample_entropies(samp_fd)\n",
    "# get_sample_hapaxes(samp_fd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set a range of sample sizes we want to generate and the number of iterations we want (i.e., number of times we want to generate a sample of each size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_ITER = 500\n",
    "SIZES = [10, 20, 50, 100, 200, 500, 1000, 2000, 5000, 10000, 20000, 50000, 100000, 200000, 500000]\n",
    "\n",
    "FREQDIST_LIST = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for curr_sfx in SFXS:\n",
    "    \n",
    "    print('\\nSUFFIX:', curr_sfx)\n",
    "\n",
    "    # Subset full df to get only the data for the current suffix.\n",
    "    curr_sfx_df = all_sfxs_df[all_sfxs_df.sfx == curr_sfx].reset_index(drop=True)\n",
    "\n",
    "    for iter_idx in range(1, NUM_ITER+1):\n",
    "        \n",
    "        for size in SIZES:\n",
    "            \n",
    "            # Draw a random sample from curr_sfx_df of size 'size'.\n",
    "            curr_rd_samp = draw_random_sample(curr_sfx_df, size)\n",
    "            \n",
    "            # Compute its freqdist and use this to compute its entropy and hapax info.\n",
    "            curr_freqdist = get_sample_freqdist(curr_rd_samp)\n",
    "            \n",
    "            # Add some more information to freqdist and then append to FREQDIST_LIST.\n",
    "            curr_freqdist['iter'] = iter_idx\n",
    "            curr_freqdist['suffix'] = curr_sfx\n",
    "            curr_freqdist['sample_size'] = size\n",
    "            curr_freqdist = curr_freqdist[['suffix', 'iter', 'sample_size', 'type', 'n_tokens', 'rank']]\n",
    "            FREQDIST_LIST.append(curr_freqdist)\n",
    "        \n",
    "        if iter_idx % 50 == 0:\n",
    "            print('  Done iter', iter_idx)\n",
    "    \n",
    "# Use pd.concat on FREQDIST_LIST, since it's a list of dfs.\n",
    "pd.concat(FREQDIST_LIST).to_csv('iterdata/freqdist_iter_500_wrepl.csv', index=False)\n",
    "\n",
    "# (The file `iterdata/freqdist_iter_500.csv` was created with exactly the same code, just with W_REPL = False.)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
