{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variables computed in this script:\n",
    "\n",
    "**Frequency ratio:** Compute the base-derivation frequency ratio for each pair and then get the average for each lemma.\n",
    "(If the derivation frequency in the column `lemma_freq` is 0, replace with 1, so that (a) the math doesn't break and (b) because the lemma is only in the sample because it did appear in DECOW16B, and the frequency counts diverge because they are from the smaller DECOW16A-NANO.)\n",
    "\n",
    "**Semantic relatedness:** The idea here is to take the average of the semantic relatedness probabilities from DErivBase for each suffix.\n",
    "To identify the base-derivation pairs that are given in DErivBase, we first extract and organise all the rows that contain derivations with the suffix we care about using `get_sfx_rows()`, and then we use the base candidates generated by `backformer_two` to select only those rows that contain actual bases for the given derivations.\n",
    "\n",
    "**Junctural phonotactics:** The token-based probability of the juncture (bigraph) at the morpheme boundary in the derived words showing up in German simplexes.\n",
    "\n",
    "**Entropy:** The dependent variable of the analysis, the measure of productivity. Larger values indicate a more evenly-spread-out distribution, which is a sign of a word formation pattern's productivity.\n",
    "\n",
    "**Other properties of the sample:** Number of tokens (i.e., sample size), number of types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import backformer_two as b\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Read in the files we'll need.\n",
    "RATIO_FILES = os.listdir('../../1_data/35_samples/6_backform_base_cutoff/')    # freq of bases and derivations\n",
    "PROBS = pd.read_csv('DErivBase-v2.0-probabilities.txt', sep=' ', header=None)  # prob of sem relatedness\n",
    "JUNC_PROBS = pd.read_csv('../simplexes/junctures_tokenbased.csv')    # probs of junctural bigraphs\n",
    "\n",
    "# Extract the list of suffixes.\n",
    "SFXS = [fn.split('_')[0] for fn in RATIO_FILES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some things that are needed for translating between my data's format and the suffixes in DErivBase,\n",
    "# and for dealing with the data in DErivBase's format.\n",
    "\n",
    "# We need a list that contains all the forms of each suffix that appear in the DErivBase data.\n",
    "# All are the same as their values in SFXS, except heit -> heit/keit, eV -> e, eA -> e, and itaet -> ität.\n",
    "SFX_LISTS = [['age'], ['ament'], ['and'], ['ant'], ['anz'], ['ateur'], ['ation'], ['ator'], ['atur'], ['e'], ['el'], \n",
    "             ['ement'], ['end'], ['ent'], ['enz'], ['er'], ['eur'], ['e'], ['heit', 'keit'], ['ie'], ['iker'], \n",
    "             ['ikum'], ['ik'], ['iment'], ['ismus'], ['ist'], ['ität'], ['iteur'], ['ition'], ['itur'], ['ium'], \n",
    "             ['ling'], ['nis'], ['schaft'], ['ung']]\n",
    "\n",
    "# Also need a list that distinguishes the -eA and -eV derivations based on the POS of their base.\n",
    "# Can leave the base POSs of all the other suffixes underspecified, since none of the others are syncretic.\n",
    "BASE_POS = [None, None, None, None, None, None, None, None, None, 'A', None, None, None, None, None, None, None, 'V', \n",
    "            None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None, None]\n",
    "\n",
    "\n",
    "def semrel_get_sfx_rows(prob_df, sfx_list, base_pos):\n",
    "    \"\"\"\n",
    "    Subsets and rearranges the DErivBase probabilities data to contain all derivations with the given\n",
    "    suffix in one column and all potential bases in another.\n",
    "    \n",
    "    Arg:\n",
    "        prob_df: pandas df containing the DErivBase v2.0 probabilities\n",
    "        sfx_list: list with string elements containign forms of the suffix\n",
    "        base_pos: either None or string representing POS of base\n",
    "    Returns:\n",
    "        Pandas df, subset + rearrangement of prob_df.\n",
    "    \"\"\"\n",
    "    assert isinstance(sfx_list, list), 'Suffixes must be passed in as lists, e.g. [\"ung\"]'\n",
    "    \n",
    "    sfx_regex = '|'.join([sfx+'_N' for sfx in sfx_list]) if len(sfx_list) > 1 else sfx_list[0]+'_N'\n",
    "    \n",
    "    # It's not the case in prob_df that all the derivs are in one col and all the bases are in the other.\n",
    "    # So select from each col individually, reorder the df where deriv is in the second column, and concat.\n",
    "    \n",
    "    # If base_pos is None, then we can select all given bases.\n",
    "    # If it's not None (i.e., for -e), then we need to make sure that the bases are labelled with the correct POS.\n",
    "    if base_pos == None:\n",
    "        sfx_in_col0 = prob_df[ prob_df[0].str.contains(sfx_regex, regex=True) ].rename(columns={0:'sfx', 1:'notsfx', 2:'prob'})\n",
    "        sfx_in_col1 = prob_df[ prob_df[1].str.contains(sfx_regex, regex=True) ].rename(columns={0:'notsfx', 1:'sfx', 2:'prob'})\n",
    "    else:\n",
    "        sfx_in_col0 = prob_df[ (prob_df[0].str.contains(sfx_regex, regex=True)) & (prob_df[1].str.contains('_' + base_pos, regex=True)) ].rename(columns={0:'sfx', 1:'notsfx', 2:'prob'})\n",
    "        sfx_in_col1 = prob_df[ (prob_df[1].str.contains(sfx_regex, regex=True)) & (prob_df[0].str.contains('_' + base_pos, regex=True)) ].rename(columns={0:'notsfx', 1:'sfx', 2:'prob'})\n",
    "    sfx_df = pd.concat([sfx_in_col0, sfx_in_col1[['sfx', 'notsfx', 'prob']]])\n",
    "    \n",
    "    # Remove the rows where both columns contain sfx_regex, since these are pairs like Aufrichtigkeit/Unaufrichtigkeit.\n",
    "    # It will be simpler if only the 'heit' column contains derivations, and we don't care about these prefixes right now \n",
    "    # anyway. Reset_index() needed for merging with \n",
    "    sfx_df = sfx_df[~sfx_df.notsfx.str.contains(sfx_regex, regex=True)].reset_index(drop=True)\n",
    "    \n",
    "    # Split deriv and other into separate columns for the word and the POS. \n",
    "    # The column containing the derivations must be called 'lemma' for Backformer to work.\n",
    "    deriv_df = pd.DataFrame(sfx_df.sfx.str.split('_').tolist(), columns = ['lemma', 'lemma_pos'])\n",
    "    other_df = pd.DataFrame(sfx_df.notsfx.str.split('_').tolist(), columns = ['other', 'other_pos'])\n",
    "    sfx_df = pd.concat([sfx_df, deriv_df, other_df], axis=1).drop(columns=['sfx', 'notsfx'])\n",
    "    \n",
    "    return sfx_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sfx</th>\n",
       "      <th>mean_freq_ratio</th>\n",
       "      <th>mean_log_freq_ratio</th>\n",
       "      <th>semrel_prob</th>\n",
       "      <th>mean_junc_prob</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>n_types</th>\n",
       "      <th>entropy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>-age</td>\n",
       "      <td>240.157098</td>\n",
       "      <td>0.901831</td>\n",
       "      <td>0.748042</td>\n",
       "      <td>0.001762</td>\n",
       "      <td>1403</td>\n",
       "      <td>14</td>\n",
       "      <td>1.901844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>-ament</td>\n",
       "      <td>0.059939</td>\n",
       "      <td>-4.032370</td>\n",
       "      <td>0.693490</td>\n",
       "      <td>0.006011</td>\n",
       "      <td>4452</td>\n",
       "      <td>3</td>\n",
       "      <td>1.147413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>-and</td>\n",
       "      <td>0.947942</td>\n",
       "      <td>-0.811246</td>\n",
       "      <td>0.833429</td>\n",
       "      <td>0.002903</td>\n",
       "      <td>37</td>\n",
       "      <td>4</td>\n",
       "      <td>1.283551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>-ant</td>\n",
       "      <td>73.311989</td>\n",
       "      <td>1.715324</td>\n",
       "      <td>0.804665</td>\n",
       "      <td>0.003625</td>\n",
       "      <td>8137</td>\n",
       "      <td>66</td>\n",
       "      <td>4.044952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>-anz</td>\n",
       "      <td>42.976578</td>\n",
       "      <td>0.789614</td>\n",
       "      <td>0.857974</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>5300</td>\n",
       "      <td>16</td>\n",
       "      <td>2.313807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>-ateur</td>\n",
       "      <td>877.290151</td>\n",
       "      <td>4.903979</td>\n",
       "      <td>0.864684</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>13671</td>\n",
       "      <td>71</td>\n",
       "      <td>2.905207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>-ation</td>\n",
       "      <td>2.693713</td>\n",
       "      <td>-1.218215</td>\n",
       "      <td>0.935052</td>\n",
       "      <td>0.002737</td>\n",
       "      <td>15824</td>\n",
       "      <td>221</td>\n",
       "      <td>5.371834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>-ator</td>\n",
       "      <td>84.806951</td>\n",
       "      <td>1.727350</td>\n",
       "      <td>0.800832</td>\n",
       "      <td>0.002619</td>\n",
       "      <td>16585</td>\n",
       "      <td>119</td>\n",
       "      <td>4.792289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>-atur</td>\n",
       "      <td>66.806245</td>\n",
       "      <td>1.324651</td>\n",
       "      <td>0.768693</td>\n",
       "      <td>0.003395</td>\n",
       "      <td>4826</td>\n",
       "      <td>16</td>\n",
       "      <td>2.392126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>-eA</td>\n",
       "      <td>34.653455</td>\n",
       "      <td>1.799621</td>\n",
       "      <td>0.910419</td>\n",
       "      <td>0.011912</td>\n",
       "      <td>754</td>\n",
       "      <td>33</td>\n",
       "      <td>4.241452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>-el</td>\n",
       "      <td>23.887259</td>\n",
       "      <td>-1.284338</td>\n",
       "      <td>0.768868</td>\n",
       "      <td>0.006768</td>\n",
       "      <td>7592</td>\n",
       "      <td>115</td>\n",
       "      <td>4.874911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>-ement</td>\n",
       "      <td>1.293510</td>\n",
       "      <td>-0.365370</td>\n",
       "      <td>0.882206</td>\n",
       "      <td>0.005504</td>\n",
       "      <td>6351</td>\n",
       "      <td>7</td>\n",
       "      <td>0.967699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>-end</td>\n",
       "      <td>1328.014187</td>\n",
       "      <td>4.815287</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002872</td>\n",
       "      <td>88</td>\n",
       "      <td>11</td>\n",
       "      <td>1.571566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>-ent</td>\n",
       "      <td>155.307441</td>\n",
       "      <td>-0.764874</td>\n",
       "      <td>0.901200</td>\n",
       "      <td>0.008567</td>\n",
       "      <td>11145</td>\n",
       "      <td>27</td>\n",
       "      <td>3.426438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>-enz</td>\n",
       "      <td>199.299923</td>\n",
       "      <td>-1.979347</td>\n",
       "      <td>0.917088</td>\n",
       "      <td>0.008034</td>\n",
       "      <td>7592</td>\n",
       "      <td>17</td>\n",
       "      <td>2.901834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>-er</td>\n",
       "      <td>212.331985</td>\n",
       "      <td>1.030233</td>\n",
       "      <td>0.811161</td>\n",
       "      <td>0.006969</td>\n",
       "      <td>9252</td>\n",
       "      <td>751</td>\n",
       "      <td>7.841041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>-eur</td>\n",
       "      <td>24.019502</td>\n",
       "      <td>0.563086</td>\n",
       "      <td>0.843944</td>\n",
       "      <td>0.008903</td>\n",
       "      <td>3225</td>\n",
       "      <td>26</td>\n",
       "      <td>3.430025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>-eV</td>\n",
       "      <td>3.548787</td>\n",
       "      <td>-1.656834</td>\n",
       "      <td>0.748682</td>\n",
       "      <td>0.008029</td>\n",
       "      <td>5222</td>\n",
       "      <td>185</td>\n",
       "      <td>6.101639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>-heit</td>\n",
       "      <td>206.979930</td>\n",
       "      <td>2.333448</td>\n",
       "      <td>0.965558</td>\n",
       "      <td>0.000053</td>\n",
       "      <td>19835</td>\n",
       "      <td>1099</td>\n",
       "      <td>6.941750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>-ie</td>\n",
       "      <td>279.756370</td>\n",
       "      <td>-0.060402</td>\n",
       "      <td>0.967168</td>\n",
       "      <td>0.003598</td>\n",
       "      <td>9155</td>\n",
       "      <td>300</td>\n",
       "      <td>5.905188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>-iker</td>\n",
       "      <td>235.501182</td>\n",
       "      <td>2.666061</td>\n",
       "      <td>0.915355</td>\n",
       "      <td>0.004163</td>\n",
       "      <td>19371</td>\n",
       "      <td>147</td>\n",
       "      <td>4.327018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>-ikum</td>\n",
       "      <td>1184.116235</td>\n",
       "      <td>3.707035</td>\n",
       "      <td>0.824149</td>\n",
       "      <td>0.003373</td>\n",
       "      <td>7870</td>\n",
       "      <td>62</td>\n",
       "      <td>2.553449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>-ik</td>\n",
       "      <td>177.648132</td>\n",
       "      <td>0.947985</td>\n",
       "      <td>0.972477</td>\n",
       "      <td>0.004863</td>\n",
       "      <td>17999</td>\n",
       "      <td>176</td>\n",
       "      <td>4.761170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>-iment</td>\n",
       "      <td>0.388768</td>\n",
       "      <td>-1.166562</td>\n",
       "      <td>0.820315</td>\n",
       "      <td>0.001199</td>\n",
       "      <td>12305</td>\n",
       "      <td>2</td>\n",
       "      <td>0.873764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>-ismus</td>\n",
       "      <td>1018.708936</td>\n",
       "      <td>3.018050</td>\n",
       "      <td>0.813058</td>\n",
       "      <td>0.004528</td>\n",
       "      <td>17334</td>\n",
       "      <td>361</td>\n",
       "      <td>6.037877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>-ist</td>\n",
       "      <td>784.775437</td>\n",
       "      <td>2.940603</td>\n",
       "      <td>0.789138</td>\n",
       "      <td>0.004172</td>\n",
       "      <td>15088</td>\n",
       "      <td>286</td>\n",
       "      <td>5.610600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>-itaet</td>\n",
       "      <td>166.557226</td>\n",
       "      <td>2.065938</td>\n",
       "      <td>0.939425</td>\n",
       "      <td>0.004381</td>\n",
       "      <td>10837</td>\n",
       "      <td>168</td>\n",
       "      <td>5.503952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>-iteur</td>\n",
       "      <td>2003.387387</td>\n",
       "      <td>4.266496</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.008671</td>\n",
       "      <td>501</td>\n",
       "      <td>3</td>\n",
       "      <td>0.073588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>-ition</td>\n",
       "      <td>0.460051</td>\n",
       "      <td>-2.868491</td>\n",
       "      <td>0.897562</td>\n",
       "      <td>0.026403</td>\n",
       "      <td>16758</td>\n",
       "      <td>16</td>\n",
       "      <td>2.906951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>-itur</td>\n",
       "      <td>57.492004</td>\n",
       "      <td>0.932244</td>\n",
       "      <td>0.867836</td>\n",
       "      <td>0.006717</td>\n",
       "      <td>11240</td>\n",
       "      <td>4</td>\n",
       "      <td>1.179115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>-ium</td>\n",
       "      <td>27.886191</td>\n",
       "      <td>0.489184</td>\n",
       "      <td>0.906172</td>\n",
       "      <td>0.044890</td>\n",
       "      <td>5171</td>\n",
       "      <td>19</td>\n",
       "      <td>1.857173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>-ling</td>\n",
       "      <td>904.179644</td>\n",
       "      <td>3.360328</td>\n",
       "      <td>0.716892</td>\n",
       "      <td>0.000660</td>\n",
       "      <td>14833</td>\n",
       "      <td>73</td>\n",
       "      <td>4.532232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>-nis</td>\n",
       "      <td>18.717619</td>\n",
       "      <td>0.218610</td>\n",
       "      <td>0.800322</td>\n",
       "      <td>0.000180</td>\n",
       "      <td>19119</td>\n",
       "      <td>53</td>\n",
       "      <td>4.203008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>-schaft</td>\n",
       "      <td>4417.826056</td>\n",
       "      <td>3.907025</td>\n",
       "      <td>0.891036</td>\n",
       "      <td>0.001789</td>\n",
       "      <td>19197</td>\n",
       "      <td>156</td>\n",
       "      <td>4.512907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>-ung</td>\n",
       "      <td>435.957198</td>\n",
       "      <td>-0.730275</td>\n",
       "      <td>0.936639</td>\n",
       "      <td>0.001943</td>\n",
       "      <td>18927</td>\n",
       "      <td>1436</td>\n",
       "      <td>8.838755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        sfx  mean_freq_ratio  mean_log_freq_ratio  semrel_prob  \\\n",
       "0      -age       240.157098             0.901831     0.748042   \n",
       "1    -ament         0.059939            -4.032370     0.693490   \n",
       "2      -and         0.947942            -0.811246     0.833429   \n",
       "3      -ant        73.311989             1.715324     0.804665   \n",
       "4      -anz        42.976578             0.789614     0.857974   \n",
       "5    -ateur       877.290151             4.903979     0.864684   \n",
       "6    -ation         2.693713            -1.218215     0.935052   \n",
       "7     -ator        84.806951             1.727350     0.800832   \n",
       "8     -atur        66.806245             1.324651     0.768693   \n",
       "9       -eA        34.653455             1.799621     0.910419   \n",
       "10      -el        23.887259            -1.284338     0.768868   \n",
       "11   -ement         1.293510            -0.365370     0.882206   \n",
       "12     -end      1328.014187             4.815287          NaN   \n",
       "13     -ent       155.307441            -0.764874     0.901200   \n",
       "14     -enz       199.299923            -1.979347     0.917088   \n",
       "15      -er       212.331985             1.030233     0.811161   \n",
       "16     -eur        24.019502             0.563086     0.843944   \n",
       "17      -eV         3.548787            -1.656834     0.748682   \n",
       "18    -heit       206.979930             2.333448     0.965558   \n",
       "19      -ie       279.756370            -0.060402     0.967168   \n",
       "20    -iker       235.501182             2.666061     0.915355   \n",
       "21    -ikum      1184.116235             3.707035     0.824149   \n",
       "22      -ik       177.648132             0.947985     0.972477   \n",
       "23   -iment         0.388768            -1.166562     0.820315   \n",
       "24   -ismus      1018.708936             3.018050     0.813058   \n",
       "25     -ist       784.775437             2.940603     0.789138   \n",
       "26   -itaet       166.557226             2.065938     0.939425   \n",
       "27   -iteur      2003.387387             4.266496          NaN   \n",
       "28   -ition         0.460051            -2.868491     0.897562   \n",
       "29    -itur        57.492004             0.932244     0.867836   \n",
       "30     -ium        27.886191             0.489184     0.906172   \n",
       "31    -ling       904.179644             3.360328     0.716892   \n",
       "32     -nis        18.717619             0.218610     0.800322   \n",
       "33  -schaft      4417.826056             3.907025     0.891036   \n",
       "34     -ung       435.957198            -0.730275     0.936639   \n",
       "\n",
       "    mean_junc_prob  n_tokens  n_types   entropy  \n",
       "0         0.001762      1403       14  1.901844  \n",
       "1         0.006011      4452        3  1.147413  \n",
       "2         0.002903        37        4  1.283551  \n",
       "3         0.003625      8137       66  4.044952  \n",
       "4         0.003383      5300       16  2.313807  \n",
       "5         0.003089     13671       71  2.905207  \n",
       "6         0.002737     15824      221  5.371834  \n",
       "7         0.002619     16585      119  4.792289  \n",
       "8         0.003395      4826       16  2.392126  \n",
       "9         0.011912       754       33  4.241452  \n",
       "10        0.006768      7592      115  4.874911  \n",
       "11        0.005504      6351        7  0.967699  \n",
       "12        0.002872        88       11  1.571566  \n",
       "13        0.008567     11145       27  3.426438  \n",
       "14        0.008034      7592       17  2.901834  \n",
       "15        0.006969      9252      751  7.841041  \n",
       "16        0.008903      3225       26  3.430025  \n",
       "17        0.008029      5222      185  6.101639  \n",
       "18        0.000053     19835     1099  6.941750  \n",
       "19        0.003598      9155      300  5.905188  \n",
       "20        0.004163     19371      147  4.327018  \n",
       "21        0.003373      7870       62  2.553449  \n",
       "22        0.004863     17999      176  4.761170  \n",
       "23        0.001199     12305        2  0.873764  \n",
       "24        0.004528     17334      361  6.037877  \n",
       "25        0.004172     15088      286  5.610600  \n",
       "26        0.004381     10837      168  5.503952  \n",
       "27        0.008671       501        3  0.073588  \n",
       "28        0.026403     16758       16  2.906951  \n",
       "29        0.006717     11240        4  1.179115  \n",
       "30        0.044890      5171       19  1.857173  \n",
       "31        0.000660     14833       73  4.532232  \n",
       "32        0.000180     19119       53  4.203008  \n",
       "33        0.001789     19197      156  4.512907  \n",
       "34        0.001943     18927     1436  8.838755  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define list that will iteratively gain a dictionary with the data for each suffix in the for loop below.\n",
    "VARS_LIST = []\n",
    "\n",
    "for idx in range(len(SFXS)):\n",
    "    curr_sfx = SFXS[idx]\n",
    "\n",
    "    # ===== Frequency ratios =====\n",
    "    \n",
    "    ratio_df = pd.read_csv('../../1_data/35_samples/6_backform_base_cutoff/' + RATIO_FILES[idx])\n",
    "    \n",
    "    # We're only interested in the subset with true_base == 1.\n",
    "    ratio_df = ratio_df[ratio_df.true_base == 1]\n",
    "    \n",
    "    # Compute freq ratio, replacing a lemma_freq of 0 with 1 to avoid divide-by-zero issues. \n",
    "    ratio_df['freq_ratio'] = pd.np.where(ratio_df.lemma_freq == 0,\n",
    "                                       ratio_df.base_freq,  # base_freq/1 = base_freq\n",
    "                                       ratio_df.base_freq / ratio_df.lemma_freq)\n",
    "    ratio_df['log_freq_ratio'] = pd.np.log(ratio_df.freq_ratio)\n",
    "    \n",
    "    \n",
    "    # ===== Semantic relatedness =====\n",
    "    \n",
    "    sfx_list = SFX_LISTS[idx]\n",
    "    base_pos = BASE_POS[idx]\n",
    "    bf_sfx = '-e' if curr_sfx in ['-eA', '-eV'] else curr_sfx  # The form required for backformer.\n",
    "    \n",
    "    # Get all probabilities of relatedness for the current suffix.\n",
    "    curr_probs = semrel_get_sfx_rows(PROBS, sfx_list, base_pos)\n",
    "    \n",
    "    # Get the base candidates from Backformer.\n",
    "    base_cands = b.get_bases_no_cleanup(curr_probs, bf_sfx)\n",
    "    \n",
    "    # We want to see if any of the values in columns beginning with 'base_cand' are the same as the value in 'other'.\n",
    "    # Use df.values to get an array of the values in each row of base_cand cols, zip tog with value of 'other', and compare.\n",
    "    base_cand_colnames = base_cands.columns[base_cands.columns.str.startswith('base_cand')]\n",
    "    base_cands['other_in_cand'] = [x[0] in x[1] for x in zip(base_cands['other'], base_cands[base_cand_colnames].values)]\n",
    "    \n",
    "    # Only select those pairs whose bases are among the candidates. This allows one derivative to appear with multiple\n",
    "    # bases, but that's OK, since sometimes it's not so clear which of clearly related bases is The True one (maybe there\n",
    "    # is no true one).\n",
    "    bases_df = base_cands[base_cands['other_in_cand']]\n",
    "    bases_df = bases_df.drop(columns = base_cand_colnames).drop(columns=['other_in_cand']).reset_index(drop=True)\n",
    "    \n",
    "    # ===== Junctural probabilities =====\n",
    "    \n",
    "    len_sfx = len(curr_sfx) - 1\n",
    "    junc_df = pd.read_csv('../../1_data/35_samples/7_analysis_samples/' + curr_sfx + '_sample.csv')\n",
    "        \n",
    "    # Identify the bigraph that spans the juncture for each lemma.\n",
    "    junc_df['bigraph'] = [x[-(len_sfx+1):-(len_sfx-1)] for x in junc_df.lemma]\n",
    "    \n",
    "    # Merge with JUNC_PROBS, and give probabilty of 0 to any junctures that don't appear in simplexes at all.\n",
    "    junc_df = pd.merge(junc_df, JUNC_PROBS[['bigraph', 'propn']], how='left', on='bigraph')\n",
    "    junc_df['junc_prob'] = junc_df['propn'].fillna(0)\n",
    "    junc_df = junc_df.drop(columns=['propn']).rename(columns={'bigraph':'juncture'})\n",
    "    \n",
    "    # ===== Shannon entropy =====\n",
    "\n",
    "    sample_df = pd.read_csv('../../1_data/35_samples/7_analysis_samples/' + curr_sfx + '_sample.csv')\n",
    "    ent = entropy(sample_df.lemma.value_counts().values, base = 2)\n",
    "    \n",
    "    # ===== Putting it all together =====\n",
    "    \n",
    "    VARS_LIST.append({'sfx': curr_sfx, \n",
    "                      'mean_freq_ratio': ratio_df.freq_ratio.mean(), \n",
    "                      'mean_log_freq_ratio': ratio_df.log_freq_ratio.mean(),\n",
    "                      'semrel_prob': bases_df.prob.mean(),\n",
    "                      'mean_junc_prob': junc_df.junc_prob.mean(),\n",
    "                      'n_tokens': len(sample_df), \n",
    "                      'n_types': len(sample_df.lemma.unique()),\n",
    "                      'entropy': ent\n",
    "                     })\n",
    "\n",
    "VARS_DF = pd.DataFrame(VARS_LIST)\n",
    "VARS_DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VARS_DF.to_csv('sfx_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
