{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in the SMOR results and use them to remove compounds from `simplex_filtered1.csv`, yielding `simplex_filtered2.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "def smored_to_dict(smor_out):\n",
    "    \"\"\"\n",
    "    Reads SMOR output into dictionary format.\n",
    "    \n",
    "    Arg:\n",
    "        smor_out: A list of lines output from SMOR.\n",
    "    Returns:\n",
    "        A dictionary in which the unique words analysed by SMOR are the keys, and the lists of analyses are the values.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create dictionary that will be iteratively expanded in the for loop below, going through each line\n",
    "    # of the SMOR output.\n",
    "    analyses = dict()\n",
    "    for line in smor_out:\n",
    "        \n",
    "        # Get the current word. (This works because the word, formatted '> Word', always precedes its analyses)\n",
    "        if line[0] == \">\":\n",
    "            curr_wd = line[2:]\n",
    "            \n",
    "            # Flag whether this word is already in the dictionary. If it is, then we don't need to re-add it.\n",
    "            new_token = False if curr_wd in analyses.keys() else True\n",
    "            \n",
    "        # If the SMOR output isn't already in analyses, then we'll add it here.\n",
    "        # Get the analyses of that word. Add to value if that key has already been made (i.e., if we're looking\n",
    "        # at line 2+ of the SMOR output); else, create key.\n",
    "        else:\n",
    "            if new_token:\n",
    "                if curr_wd in analyses.keys():\n",
    "                    analyses[curr_wd].append(line)\n",
    "                else:\n",
    "                    analyses[curr_wd] = [line]\n",
    "            \n",
    "    return analyses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1:** Convert SMOR output to a dictionary and trim it to remove unparsable words, NN compounds, and a few other things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13864"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in the SMOR output.\n",
    "with open('infiles/simplex_filtered1.smored', encoding='utf-8') as file:\n",
    "    smored = [line.strip() for line in file]\n",
    "\n",
    "smored_dict = smored_to_dict(smored)\n",
    "len(smored_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of `smored_dict` is less than the length of the original file because of words that have the same form but different POSs. \n",
    "Since, in the end, we only care about string matching, it's OK to lose these duplicates at this point.\n",
    "\n",
    "We can safely ignore anything that can't be analysed by SMOR, indicated by a value containing `no result for`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12162"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove no-result-for items.\n",
    "smored_dict = {key:val for key, val in smored_dict.items() if not \"no result for\" in val[0]}\n",
    "len(smored_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We particularly want to identify NN compounds, and we can find them by matching a sequence of `<NN>` followed by `<+NN>` (indicating a noun followed by a head noun)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8503"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smored_dict = {key:val for key, val in smored_dict.items() if re.search('<NN>.*<\\+NN>', \"\".join(val)) == None}\n",
    "# (The join() call combines all analyses in the val list into one string for easier searching)\n",
    "len(smored_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also remove any words that contain numerals or punctuation and any that are only one character long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8462"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smored_dict = {key:val for key, val in smored_dict.items() if re.search('\\d', key) == None}\n",
    "len(smored_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8292"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smored_dict = {key:val for key, val in smored_dict.items() if not any(punct in key for punct in string.punctuation)}\n",
    "len(smored_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8268"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smored_dict = {key:val for key, val in smored_dict.items() if len(key) > 1}\n",
    "len(smored_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ This is how many unique lemmata there are to annotate.\n",
    "\n",
    "**Part 2:** Read in `outfiles/simplex_filtered1.csv`, only keep the rows in which the value of the `lemma` column is in `smored_dict.keys()`, and save as `outfiles/simplex_filtered2.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt1 = pd.read_csv('outfiles/simplex_filtered1.csv')\n",
    "filt2 = filt1[ filt1['lemma'].isin(smored_dict.keys()) ]\n",
    "filt2.to_csv('outfiles/simplex_filtered2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also dedup the lemmata for slightly more efficient annotation; this will be merged with `outfiles/simplex_filtered2.csv` in the next step to produce `outfiles/simplex_filtered3.csv`.\n",
    "The annotation takes place in `infiles/simplex_filtered2_annotated.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(filt2.lemma.unique(), name='lemma').to_csv('outfiles/simplex_filtered2_toannot.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
